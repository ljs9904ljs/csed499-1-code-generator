/home/junseok/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
11/06/2023 00:29:12 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 2distributed training: False, 16-bits training: False
11/06/2023 00:29:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm/runs/Nov06_00-29-11_n108.cluster,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/tmp/test-clm,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-clm,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-ca6326f451e387e1
11/06/2023 00:29:12 - INFO - datasets.builder - Using custom data configuration default-ca6326f451e387e1
Loading Dataset Infos from /home/junseok/.local/lib/python3.10/site-packages/datasets/packaged_modules/text
11/06/2023 00:29:12 - INFO - datasets.info - Loading Dataset Infos from /home/junseok/.local/lib/python3.10/site-packages/datasets/packaged_modules/text
Overwrite dataset info from restored data version if exists.
11/06/2023 00:29:12 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
11/06/2023 00:29:12 - INFO - datasets.info - Loading Dataset info from /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
Found cached dataset text (/home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
11/06/2023 00:29:12 - INFO - datasets.builder - Found cached dataset text (/home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
Loading Dataset info from /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
11/06/2023 00:29:12 - INFO - datasets.info - Loading Dataset info from /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
[INFO|configuration_utils.py:717] 2023-11-06 00:29:13,617 >> loading configuration file config.json from cache at /home/junseok/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2023-11-06 00:29:13,619 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_auto.py:566] 2023-11-06 00:29:13,854 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:717] 2023-11-06 00:29:14,073 >> loading configuration file config.json from cache at /home/junseok/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2023-11-06 00:29:14,074 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2023-11-06 00:29:14,935 >> loading file vocab.json from cache at /home/junseok/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json
[INFO|tokenization_utils_base.py:2022] 2023-11-06 00:29:14,935 >> loading file merges.txt from cache at /home/junseok/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt
[INFO|tokenization_utils_base.py:2022] 2023-11-06 00:29:14,935 >> loading file tokenizer.json from cache at /home/junseok/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2023-11-06 00:29:14,935 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2023-11-06 00:29:14,935 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2023-11-06 00:29:14,935 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:717] 2023-11-06 00:29:14,935 >> loading configuration file config.json from cache at /home/junseok/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2023-11-06 00:29:14,936 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:3121] 2023-11-06 00:29:15,055 >> loading weights file model.safetensors from cache at /home/junseok/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors
[INFO|configuration_utils.py:791] 2023-11-06 00:29:15,062 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:3950] 2023-11-06 00:29:17,886 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:3958] 2023-11-06 00:29:17,886 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:751] 2023-11-06 00:29:18,116 >> loading configuration file generation_config.json from cache at /home/junseok/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/generation_config.json
[INFO|configuration_utils.py:791] 2023-11-06 00:29:18,117 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-e032f323f7702f20.arrow
11/06/2023 00:29:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-e032f323f7702f20.arrow
Loading cached processed dataset at /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-212799b5ad6e5f9f.arrow
11/06/2023 00:29:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-212799b5ad6e5f9f.arrow
Loading cached processed dataset at /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-cc877396054df122.arrow
11/06/2023 00:29:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-cc877396054df122.arrow
Loading cached processed dataset at /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-5b9d1647d8ddfef5.arrow
11/06/2023 00:29:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/junseok/.cache/huggingface/datasets/text/default-ca6326f451e387e1/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-5b9d1647d8ddfef5.arrow
11/06/2023 00:29:19 - WARNING - accelerate.utils.other - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:1723] 2023-11-06 00:29:19,711 >> ***** Running training *****
[INFO|trainer.py:1724] 2023-11-06 00:29:19,711 >>   Num examples = 22,467
[INFO|trainer.py:1725] 2023-11-06 00:29:19,711 >>   Num Epochs = 3
[INFO|trainer.py:1726] 2023-11-06 00:29:19,711 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1728] 2023-11-06 00:29:19,712 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:1729] 2023-11-06 00:29:19,712 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1730] 2023-11-06 00:29:19,712 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1731] 2023-11-06 00:29:19,712 >>   Total optimization steps = 4,215
[INFO|trainer.py:1732] 2023-11-06 00:29:19,712 >>   Number of trainable parameters = 124,439,808
  0%|          | 0/4215 [00:00<?, ?it/s]